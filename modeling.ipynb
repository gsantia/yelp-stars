{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Upon the start of the term and this project I had an intermediate skill level in regards to general Python scripting, an extremely rudimentary understanding of Machine Learning, and an intermediate level of understanding of the basic techniques and tools of data scientists. I had been using Python for about 1.5 years, had been studying data science for about half a year, and had really no formal experience with Machine Learning. Fortunately, the process of completing this project helped noticeably improve my proficiency in all three of these domains. In particular, this was the first project I took part in where I employed Scikit-Learn and employed several Machine Learning models, bringing me from absolute Machine Learning novice firmly to a beginner. Additionally, I had studied pandas on my own time in the previous quarter, and felt that this project yielded the perfect opportunity to actually apply my new-found skills to something. I used pandas DataFrames to hold the features for each training instance which I thought may be useful for classification. I also figured out how to easily use pandas in conjunction with Scikit-Learn, which I know will be highly useful for quite some time. Once getting the data into the appropriate form for use with the various Scikit-Learn Machine Learning algorithm classes (converting DataFrames to numpy arrays is extremely easy), I tried to find the best classification algorithm for assigning a text review of a business a star rating (from one to five). This turned out to be much more complicated than the usual binary classification problem, which is all I had worked with up till this point. Thus this process gave me my first real experience with multi-class classification, and showed me which Machine Learning classification algorithms are compatible with the multi-class problem. In addition, given the ordinal nature of the problem (_2 stars_ is much closer to _1 star_ than it is to _5 stars_), it also gave me insight into various algorithms for classification of ordinal classes. I read mostly about ordinal regression, which I found isn't currently employed in Scikit-Learn. Thus I employed the third-party module mord to perform a rudimentary ordinal regression. Looking forward, I hope to perform many more projects such as this. Perhaps I may even continue this project and manage to make it into work decent enough to publish a paper on! In terms of self-study, I plan to begin studying TensorFlow and neural networks very soon, along with continuing my studies of more advanced Python techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "We need to get the data into numpy arrays. Luckily, pandas makes it extremely easy to convert a DataFrame to an array, using the values attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>length</th>\n",
       "      <th>one_interesting</th>\n",
       "      <th>two_interesting</th>\n",
       "      <th>three_interesting</th>\n",
       "      <th>four_interesting</th>\n",
       "      <th>five_interesting</th>\n",
       "      <th>punc</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132541</th>\n",
       "      <td>3</td>\n",
       "      <td>327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.523750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231497</th>\n",
       "      <td>4</td>\n",
       "      <td>483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.205510</td>\n",
       "      <td>0.353375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43791</th>\n",
       "      <td>5</td>\n",
       "      <td>743</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.259984</td>\n",
       "      <td>0.366984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>2</td>\n",
       "      <td>1412</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.068792</td>\n",
       "      <td>0.464641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357253</th>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>0.544444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73016</th>\n",
       "      <td>3</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.576623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67692</th>\n",
       "      <td>4</td>\n",
       "      <td>1741</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.281471</td>\n",
       "      <td>0.512352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299721</th>\n",
       "      <td>1</td>\n",
       "      <td>1241</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100668</td>\n",
       "      <td>0.413503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320024</th>\n",
       "      <td>4</td>\n",
       "      <td>1313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.033741</td>\n",
       "      <td>0.668297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266360</th>\n",
       "      <td>5</td>\n",
       "      <td>829</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>0.550926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        stars  length  one_interesting  two_interesting  three_interesting  \\\n",
       "132541      3     327                1                0                  0   \n",
       "231497      4     483                0                0                  0   \n",
       "43791       5     743                1                0                  0   \n",
       "16507       2    1412                5                0                  0   \n",
       "357253      1     233                1                0                  0   \n",
       "73016       3    1028                0                0                  0   \n",
       "67692       4    1741                3                0                  1   \n",
       "299721      1    1241                3                2                  0   \n",
       "320024      4    1313                0                0                  0   \n",
       "266360      5     829                3                1                  0   \n",
       "\n",
       "        four_interesting  five_interesting  punc  sentiment  subjectivity  \n",
       "132541                 0                 0     5   0.307500      0.523750  \n",
       "231497                 0                 1     0   0.205510      0.353375  \n",
       "43791                  0                 3    11   0.259984      0.366984  \n",
       "16507                  0                 9    21   0.068792      0.464641  \n",
       "357253                 0                 0     3   0.105556      0.544444  \n",
       "73016                  0                 1    21   0.172989      0.576623  \n",
       "67692                  2                 0    20   0.281471      0.512352  \n",
       "299721                 1                 1     5   0.100668      0.413503  \n",
       "320024                 2                 0    11   0.033741      0.668297  \n",
       "266360                 0                 3     2   0.307685      0.550926  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yolo = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.00000000e+00,  3.27000000e+02,  1.00000000e+00, ...,\n",
       "         5.00000000e+00,  3.07500000e-01,  5.23750000e-01],\n",
       "       [ 4.00000000e+00,  4.83000000e+02,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  2.05509642e-01,  3.53374656e-01],\n",
       "       [ 5.00000000e+00,  7.43000000e+02,  1.00000000e+00, ...,\n",
       "         1.10000000e+01,  2.59984127e-01,  3.66984127e-01],\n",
       "       ...,\n",
       "       [ 5.00000000e+00,  9.50000000e+01,  0.00000000e+00, ...,\n",
       "         2.00000000e+00,  6.00000000e-01,  7.70000000e-01],\n",
       "       [ 4.00000000e+00,  8.00000000e+02,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  1.45553221e-01,  4.45378151e-01],\n",
       "       [ 1.00000000e+00,  8.05000000e+02,  1.00000000e+00, ...,\n",
       "         8.00000000e+00, -2.27678571e-01,  5.15773810e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars = train['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The stars are what we're trying to find, so we definitely need to remove that from the training set\n",
    "del train['stars']\n",
    "X = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378951, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378951,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.2700e+02, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 5.0000e+00, 3.0750e-01, 5.2375e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, ..., 5, 4, 1], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data makes many of the algorithms in Scikit-Learn more precise. It is interesting to note that Random Forests aren't affected by this preprocessing at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "X_test_scaled = scaler.fit_transform(X_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform classification\n",
    "\n",
    "I'm really not sure which algorithms to use here, so I think it may be best just to try some out and see which works best. This is not a binary classification, so we are limited to a subset of the available Scikit-Learn algorithms which support multi-class classification. Since we've already munged the data, fit it into numpy arrays, and standardized it, performing the machine learning itself is a breeze. After we get some baseline results for each algorithm, I plan to select the ones that perform well, tune their hyperparameters, and then create a hard-voting ensemble to maximize our performance.\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4928091236010471"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's just try a Random Forest and see what happens\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state = 0, n_estimators = 20)\n",
    "scores = cross_val_score(forest_clf, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.493765750550857\n",
      "[0.60486255 0.17073895 0.18828674 0.32049361 0.65530058]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on the test data\n",
    "from sklearn.metrics import f1_score\n",
    "forest_clf.fit(X_train_scaled, y_train)\n",
    "y_pred = forest_clf.predict(X_test_scaled)\n",
    "print(f1_score(y_pred, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred, y_test, average = None, labels = [1, 2, 3, 4, 5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty decent. I'm under the impression that as long as it's above the baseline of randomly guessing, including it in an ensemble will improve results (law of large numbers?). Looks like we're much better at getting 1 or 5 star reviews correct than the middle ones. This definitely makes sense to me, since these are the least represented classes, and there were very few interesting 2 and 3 star words compared to the remaining categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "For each algorithm, perform a grid search in order to find the best hyperparameters. This will take a long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [1, 5, 10, 20], 'max_depth': [3, None], 'max_features': [1, 3, 9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "forest = RandomForestClassifier(random_state = 0)\n",
    "param_grid = {\"n_estimators\" : [1, 5, 10, 20],\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 9]}\n",
    "\n",
    "grid_search = GridSearchCV(forest, param_grid = param_grid)\n",
    "grid_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'max_features': 9, 'n_estimators': 20}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053173578351511"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ok, now train the forest classifier again with the best params\n",
    "forest = RandomForestClassifier(max_depth = 3, max_features = 9, n_estimators = 20, random_state = 0)\n",
    "scores = cross_val_score(forest, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5055349579765408\n",
      "[0.60796471 0.         0.         0.28187404 0.68491555]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "forest.fit(X_train_scaled, y_train)\n",
    "y_pred = forest.predict(X_test_scaled)\n",
    "print(f1_score(y_pred, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred, y_test, average = None, labels = [1, 2, 3, 4, 5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5254057206996384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "log_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "scores2 = cross_val_score(log_clf, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores2.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52694769, 0.52516657, 0.52665083, 0.52406241, 0.52419435,\n",
       "       0.52557971, 0.52490434, 0.52597895, 0.52502227, 0.52555009])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5259859350054756\n",
      "[0.61783984 0.07250418 0.11152148 0.26508059 0.68794503]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = log_clf.predict(X_test_scaled)\n",
    "print(f1_score(y_pred2, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred2, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here looks good. This seems to the best at identifying 1 and 5 star reviews.\n",
    "\n",
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid2 = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "log = LogisticRegression()\n",
    "grid_search2 = GridSearchCV(log, param_grid=param_grid2)\n",
    "grid_search2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210515994836349"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression(C = 10)\n",
    "log.fit(X_train_scaled, y_train)\n",
    "\n",
    "scores2 = cross_val_score(log, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522291564961539\n",
      "[0.61921738 0.03388823 0.0688295  0.23749865 0.68414029]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = log.predict(X_test_scaled)\n",
    "print(f1_score(y_pred2, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred2, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like grid search actually gave us a slightly lower result on the test set? Interesting, but it's miniscule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc_clf = LinearSVC(multi_class = 'crammer_singer')\n",
    "svc_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred3 = svc_clf.predict(X_test)\n",
    "print(f1_score(y_pred3, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred3, y_test, average = None, labels = [1, 2, 3, 4, 5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is worse than just randomly guessing, and it took forever! Not a good choice. (I ran this and it gave me something like 17%. It took several hours to do. The result is gone now and I don't think re-running it is worth the time, so I'll just leave the code here).\n",
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4699235133137211"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "ber_clf = BernoulliNB()\n",
    "ber_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "scores3 = cross_val_score(ber_clf, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4704384425591429\n",
      "[0.4439941  0.07918196 0.11491012 0.18343751 0.65270592]\n"
     ]
    }
   ],
   "source": [
    "y_pred4 = ber_clf.predict(X_test_scaled)\n",
    "print(f1_score(y_pred4, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred4, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.1, 1, 3, 5, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "ber = BernoulliNB()\n",
    "param_grid3 = {'alpha' : [0.1, 1, 3, 5, 10]}\n",
    "grid_search3 = GridSearchCV(ber, param_grid = param_grid3)\n",
    "grid_search3.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4699235133137211"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ber = BernoulliNB(alpha = 0.1)\n",
    "ber.fit(X_train_scaled, y_train)\n",
    "\n",
    "scores3 = cross_val_score(ber, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4704384425591429\n",
      "[0.4439941  0.07918196 0.11491012 0.18343751 0.65270592]\n"
     ]
    }
   ],
   "source": [
    "y_pred4 = ber.predict(X_test_scaled)\n",
    "print(f1_score(y_pred4, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred4, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB\n",
    "\n",
    "I'm under the impression that Gaussian NB has no hyperparameters to mess with, so we don't have to optimize this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4721731156566008"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gauss_clf = GaussianNB()\n",
    "gauss_clf.fit(X_train_scaled, y_train)\n",
    "scores4 = cross_val_score(gauss_clf, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "scores4.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47262867622804816\n",
      "[0.50651904 0.1555837  0.16765441 0.15225431 0.63409363]\n"
     ]
    }
   ],
   "source": [
    "y_pred5 = gauss_clf.predict(X_test_scaled)\n",
    "print(f1_score(y_pred5, y_test, average = 'micro'))\n",
    "print(f1_score(y_pred5, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('forest', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features=9, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight...iNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)), ('gauss', GaussianNB(priors=None))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vote_clf = VotingClassifier(estimators = [\n",
    "    ('forest', forest),\n",
    "    ('log', log),\n",
    "    ('ber', ber),\n",
    "    ('gauss', gauss_clf)\n",
    "], voting = 'hard')\n",
    "\n",
    "vote_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5170270876489292\n",
      "[0.59534308 0.06287754 0.1056527  0.22495395 0.68361429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "final_pred = vote_clf.predict(X_test_scaled)\n",
    "print(f1_score(final_pred, y_test, average = 'micro'))\n",
    "print(f1_score(final_pred, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5156452378747759"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores = cross_val_score(vote_clf, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "final_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the ensemble seems to be a bit worse than some of our previously established methods. Perhaps this could be made better with a bit of tuning? Perhaps I could try soft-voting instead of hard in the future. Still, it feels cool to have set up my own ensemble. Scikit-Learn makes it much easier than it would be otherwise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Regression\n",
    "This isn't supported by SciKit-Learn, but I found a third party package --- _mord_ --- that claims to do it in a similar fashion.\n",
    "The documentation for this module is EXTREMELY barebones, so I'm not sure if I'll even be able to get it to work. Might as well give it a shot. I'll try out each of the algorithms that are provided in the module (that I can get to work, some of them seem to not exist anymore?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticIT(alpha=1.0, max_iter=1000, verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mord\n",
    "\n",
    "ord_clf = mord.LogisticIT()\n",
    "ord_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506887452170471"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_scores = cross_val_score(ord_clf, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "ord_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5066828515259068\n",
      "[0.58639519 0.         0.         0.20384836 0.68965432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "ord_pred = ord_clf.predict(X_test_scaled)\n",
    "print(f1_score(ord_pred, y_test, average = 'micro'))\n",
    "print(f1_score(ord_pred, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticAT(alpha=1.0, max_iter=1000, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_clf2 = mord.LogisticAT()\n",
    "ord_clf2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46190790341733734"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_scores2 = cross_val_score(ord_clf2, X_train_scaled, y_train, cv = 10, scoring = 'f1_micro')\n",
    "ord_scores2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4620601390666438\n",
      "[0.49196267 0.21603734 0.21226194 0.37056341 0.64304772]\n"
     ]
    }
   ],
   "source": [
    "ord_pred2 = ord_clf2.predict(X_test_scaled)\n",
    "print(f1_score(ord_pred2, y_test, average = 'micro'))\n",
    "print(f1_score(ord_pred2, y_test, average = None, labels = [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if I'm doing something wrong here, and there are no instructions whatsoever in the documentation. But these seem to be performing worse than the ensemble we've already created. Perhaps sometime in a future I can figure out a way to get these to perform better. It is interesting to note this model is performing by far the best on 2 and 3 star reviews out of all the ones we've tried!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
